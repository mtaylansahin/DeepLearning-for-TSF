{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab72d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Input, Flatten, Conv1D, MaxPool1D, Bidirectional, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36cafd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(input_shape,\n",
    "        horizon=1,\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"mae\",\n",
    "        hidden_layers=[32, 16, 8],\n",
    "        dropout=0.0):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Flatten 2d input into 1d\n",
    "    model.add(Flatten(input_shape=(input_shape[-2], input_shape[-1])))\n",
    "    \n",
    "    # Add Dense + Dropout for each hidden layer\n",
    "    for hidden_units in hidden_layers:\n",
    "        model.add(Dense(hidden_units))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))\n",
    "    \n",
    "    # Output layer to predict {horizon} time steps ahead\n",
    "    model.add(Dense(horizon))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(input_shape,\n",
    "        horizon=1,\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"mae\",\n",
    "        conv_blocks=[[64, 7, 2], [128, 5, 2]],\n",
    "        dense_layers=[],\n",
    "        dense_dropout=0.0):\n",
    "    \n",
    "    # Split conv_blocks into three for easier use\n",
    "    conv_layers, kernels, pool_sizes = [], [], []\n",
    "    for layer, kern, pool in conv_blocks:\n",
    "        conv_layers.append(layer)\n",
    "        kernels.append(kern)\n",
    "        pool_sizes.append(pool)\n",
    "\n",
    "    model = Sequential()\n",
    "    # First convolutional layer and pooling layer\n",
    "    model.add(Conv1D(conv_layers[0], kernels[0], \n",
    "                     activation=\"relu\", padding=\"same\", \n",
    "                     input_shape=(input_shape[-2], input_shape[-1])))\n",
    "    \n",
    "    model.add(MaxPool1D(pool_size=pool_sizes[0]))\n",
    "    \n",
    "    # Rest of the conv + pool layers\n",
    "    for chanels, kernel, pool_size in zip(conv_layers[1:], kernels[1:], pool_sizes[1:]):\n",
    "        model.add(Conv1D(chanels, kernel, activation=\"relu\", padding=\"same\"))\n",
    "        if pool_size:\n",
    "            model.add(MaxPool1D(pool_size=pool_size))\n",
    "    \n",
    "    # Flatten input for dense layers\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Dense layers with dropout\n",
    "    for hidden_units in dense_layers:\n",
    "        model.add(Dense(hidden_units))\n",
    "        if dense_dropout > 0:\n",
    "            model.add(Dropout(dense_dropout))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(horizon))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a360ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(input_shape,\n",
    "        horizon=1,\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"mae\",\n",
    "        lstm_layers=[50],\n",
    "        lstm_dropout=0.0,\n",
    "        return_sequences=False,\n",
    "        dense_layers=[],\n",
    "        dense_dropout=0.0):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Return sequences true if there are more LSTM layers after this one\n",
    "    return_seq = return_sequences if len(lstm_layers) == 1 else True\n",
    "    # First LSTM layer\n",
    "    model.add(LSTM(lstm_layers[0], activation='relu', return_sequences=return_seq, \n",
    "                   dropout=lstm_dropout, input_shape=(input_shape[-2], input_shape[-1])))\n",
    "    \n",
    "    # Rest of the LSTM layers\n",
    "    for i, u in enumerate(lstm_layers[1:]):\n",
    "        return_seq = return_sequences if i == len(lstm_layers) - 2 else True\n",
    "        model.add(LSTM(u, activation='relu', return_sequences=return_seq, dropout=lstm_dropout))\n",
    "    \n",
    "    if return_sequences:\n",
    "        model.add(Flatten())\n",
    "    \n",
    "    # Add dense layers and dropout\n",
    "    for units in dense_layers:\n",
    "        model.add(Dense(units))\n",
    "        if dense_dropout > 0:\n",
    "            model.add(Dropout(dense_dropout))\n",
    "    \n",
    "    # Output dense layer\n",
    "    model.add(Dense(horizon))\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_lstm(input_shape,\n",
    "        horizon=1,\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"mae\",\n",
    "        lstm_layers=[50],\n",
    "        lstm_dropout=0.0,\n",
    "        return_sequences=False,\n",
    "        dense_layers=[],\n",
    "        dense_dropout=0.0):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Return sequences true if there are more LSTM layers after this one\n",
    "    return_seq = return_sequences if len(lstm_layers) == 1 else True\n",
    "    # First LSTM layer\n",
    "    model.add(Bidirectional(LSTM(lstm_layers[0], activation='relu', return_sequences=return_seq, \n",
    "                   dropout=lstm_dropout), input_shape=(input_shape[-2], input_shape[-1]))))\n",
    "    \n",
    "    # Rest of the LSTM layers\n",
    "    for i, u in enumerate(lstm_layers[1:]):\n",
    "        return_seq = return_sequences if i == len(lstm_layers) - 2 else True\n",
    "        model.add(Bidirectional(LSTM(u, activation='relu', return_sequences=return_seq, dropout=lstm_dropout)))\n",
    "    \n",
    "    if return_sequences:\n",
    "        model.add(Flatten())\n",
    "    \n",
    "    # Add dense layers and dropout\n",
    "    for units in dense_layers:\n",
    "        model.add(Dense(units))\n",
    "        if dense_dropout > 0:\n",
    "            model.add(Dropout(dense_dropout))\n",
    "    \n",
    "    # Output dense layer\n",
    "    model.add(Dense(horizon))\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ba63dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(input_shape,\n",
    "        horizon=1,\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"mae\",\n",
    "        gru_layers=[50],\n",
    "        gru_dropout=0.0,\n",
    "        return_sequences=False,\n",
    "        dense_layers=[],\n",
    "        dense_dropout=0.0):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Return sequences true if there are more GRU layers after this one\n",
    "    return_seq = return_sequences if len(gru_layers) == 1 else True\n",
    "    # First GRU layer\n",
    "    model.add(GRU(gru_layers[0], activation='relu', return_sequences=return_seq, \n",
    "                   dropout=gru_dropout, input_shape=(input_shape[-2], input_shape[-1])))\n",
    "    \n",
    "    # Rest of the GRU layers\n",
    "    for i, u in enumerate(gru_layers[1:]):\n",
    "        return_seq = return_sequences if i == len(gru_layers) - 2 else True\n",
    "        model.add(GRU(u, activation='relu', return_sequences=return_seq, dropout=gru_dropout))\n",
    "    \n",
    "    if return_sequences:\n",
    "        model.add(Flatten())\n",
    "    \n",
    "    # Add dense layers and dropout\n",
    "    for units in dense_layers:\n",
    "        model.add(Dense(units))\n",
    "        if dense_dropout > 0:\n",
    "            model.add(Dropout(dense_dropout))\n",
    "    \n",
    "    # Output dense layer\n",
    "    model.add(Dense(horizon))\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder_lstm(input_shape,\n",
    "        horizon=1,\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"mae\",\n",
    "        encoder_layers=[100],\n",
    "        encoder_dropout=0.0,\n",
    "        decoder_layers=[100],\n",
    "        decoder_dropout=0.0):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Return sequences true if there are more encoder LSTM layers after this one\n",
    "    return_seq = False if len(encoder_layers) == 1 else True\n",
    "    # First encoder LSTM layer\n",
    "    model.add(LSTM(lstm_layers[0], activation='relu', return_sequences=return_seq, \n",
    "                   dropout=lstm_dropout), input_shape=(input_shape[-2], input_shape[-1]))\n",
    "    \n",
    "    # Rest of the encoder LSTM layers\n",
    "    for i, u in enumerate(encoder_layers[1:]):\n",
    "        return_seq = False if i == len(encoder_layers) - 2 else True\n",
    "        model.add(LSTM(u, activation='relu', return_sequences=return_seq, dropout=encoder_dropout))\n",
    "    \n",
    "    model.add(RepeatVector(horizon))\n",
    "    \n",
    "    # Decoder LSTM layers\n",
    "    for i, u in enumerate(decoder_layers):\n",
    "        model.add(LSTM(u, activation='relu', return_sequences=True, dropout=decoder_dropout))\n",
    "    \n",
    "    # Output dense layer\n",
    "    model.add(TimeDistributed(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
